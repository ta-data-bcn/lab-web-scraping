{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n            David Tolnay\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "soup.select(\"#pa-dtolnay > div.d-sm-flex.flex-auto > div.col-sm-8.d-md-flex > div:nth-child(1) > h1 > a\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = []\n",
    "\n",
    "for names in soup.select(\"div.col-sm-8.d-md-flex > div:nth-child(1) > h1 > a\"):\n",
    "    names_list.append(names.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n            David Tolnay\\n',\n",
       " '\\n            Stephen Celis\\n',\n",
       " '\\n            Henrik Rydgård\\n',\n",
       " '\\n            Stefan Prodan\\n',\n",
       " '\\n            Matthias Urhahn\\n',\n",
       " '\\n            Shreyas Patil\\n',\n",
       " '\\n            Mr.doob\\n',\n",
       " '\\n            Christian Muehlhaeuser\\n',\n",
       " '\\n            Anuken\\n',\n",
       " '\\n            Mathew Sachin\\n',\n",
       " '\\n            Laurent\\n',\n",
       " '\\n            cketti\\n',\n",
       " '\\n            Andrei Neagoie\\n',\n",
       " '\\n            Arjay Angeles\\n',\n",
       " '\\n            Zen Monk Alain M. Lafon\\n',\n",
       " '\\n            yhirose\\n',\n",
       " '\\n            Jared Palmer\\n',\n",
       " '\\n            PySimpleGUI\\n',\n",
       " '\\n            Andrew Gallant\\n',\n",
       " '\\n            John Arundel\\n',\n",
       " '\\n            Sindre Sorhus\\n',\n",
       " '\\n            Florian Märkl\\n',\n",
       " '\\n            Hiroshi Kimura\\n',\n",
       " '\\n            John Wiegley\\n',\n",
       " '\\n            CrazyMax\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = []\n",
    "\n",
    "for names in names_list:\n",
    "    new_names.append(names.replace(\"\\n\", \"\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['David Tolnay',\n",
       " 'Stephen Celis',\n",
       " 'Henrik Rydgård',\n",
       " 'Stefan Prodan',\n",
       " 'Matthias Urhahn',\n",
       " 'Shreyas Patil',\n",
       " 'Mr.doob',\n",
       " 'Christian Muehlhaeuser',\n",
       " 'Anuken',\n",
       " 'Mathew Sachin',\n",
       " 'Laurent',\n",
       " 'cketti',\n",
       " 'Andrei Neagoie',\n",
       " 'Arjay Angeles',\n",
       " 'Zen Monk Alain M. Lafon',\n",
       " 'yhirose',\n",
       " 'Jared Palmer',\n",
       " 'PySimpleGUI',\n",
       " 'Andrew Gallant',\n",
       " 'John Arundel',\n",
       " 'Sindre Sorhus',\n",
       " 'Florian Märkl',\n",
       " 'Hiroshi Kimura',\n",
       " 'John Wiegley',\n",
       " 'CrazyMax']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n              dtolnay\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"div.col-sm-8.d-md-flex > div:nth-child(1) > p > a\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nick_names = []\n",
    "\n",
    "for nick in soup.select(\"div.col-sm-8.d-md-flex > div:nth-child(1) > p > a\"):\n",
    "    nick_names.append(nick.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n              dtolnay\\n',\n",
       " '\\n              stephencelis\\n',\n",
       " '\\n              hrydgard\\n',\n",
       " '\\n              stefanprodan\\n',\n",
       " '\\n              d4rken\\n',\n",
       " '\\n              PatilShreyas\\n',\n",
       " '\\n              mrdoob\\n',\n",
       " '\\n              muesli\\n',\n",
       " '\\n              MathewSachin\\n',\n",
       " '\\n              laurent22\\n',\n",
       " '\\n              cketti\\n',\n",
       " '\\n              aneagoie\\n',\n",
       " '\\n              yajra\\n',\n",
       " '\\n              munen\\n',\n",
       " '\\n              jaredpalmer\\n',\n",
       " '\\n              PySimpleGUI\\n',\n",
       " '\\n              BurntSushi\\n',\n",
       " '\\n              bitfield\\n',\n",
       " '\\n              sindresorhus\\n',\n",
       " '\\n              thestr4ng3r\\n',\n",
       " '\\n              muukii\\n',\n",
       " '\\n              jwiegley\\n',\n",
       " '\\n              crazy-max\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nick_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nick_names = []\n",
    "\n",
    "for new_nick in new_nick_name:\n",
    "    new_nick_names.append(new_nick.replace(\"\\n\", \"\").lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dtolnay',\n",
       " 'stephencelis',\n",
       " 'hrydgard',\n",
       " 'stefanprodan',\n",
       " 'd4rken',\n",
       " 'PatilShreyas',\n",
       " 'mrdoob',\n",
       " 'muesli',\n",
       " 'MathewSachin',\n",
       " 'laurent22',\n",
       " 'cketti',\n",
       " 'aneagoie',\n",
       " 'yajra',\n",
       " 'munen',\n",
       " 'jaredpalmer',\n",
       " 'PySimpleGUI',\n",
       " 'BurntSushi',\n",
       " 'bitfield',\n",
       " 'sindresorhus',\n",
       " 'thestr4ng3r',\n",
       " 'muukii',\n",
       " 'jwiegley',\n",
       " 'crazy-max']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_nick_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "\n",
    "for name, nick in zip(new_names, new_nick_names):\n",
    "    new_list.append((name, \"(\" + nick + \")\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('David Tolnay', '(dtolnay)'),\n",
       " ('Stephen Celis', '(stephencelis)'),\n",
       " ('Henrik Rydgård', '(hrydgard)'),\n",
       " ('Stefan Prodan', '(stefanprodan)'),\n",
       " ('Matthias Urhahn', '(d4rken)'),\n",
       " ('Shreyas Patil', '(PatilShreyas)'),\n",
       " ('Mr.doob', '(mrdoob)'),\n",
       " ('Christian Muehlhaeuser', '(muesli)'),\n",
       " ('Anuken', '(MathewSachin)'),\n",
       " ('Mathew Sachin', '(laurent22)'),\n",
       " ('Laurent', '(cketti)'),\n",
       " ('cketti', '(aneagoie)'),\n",
       " ('Andrei Neagoie', '(yajra)'),\n",
       " ('Arjay Angeles', '(munen)'),\n",
       " ('Zen Monk Alain M. Lafon', '(jaredpalmer)'),\n",
       " ('yhirose', '(PySimpleGUI)'),\n",
       " ('Jared Palmer', '(BurntSushi)'),\n",
       " ('PySimpleGUI', '(bitfield)'),\n",
       " ('Andrew Gallant', '(sindresorhus)'),\n",
       " ('John Arundel', '(thestr4ng3r)'),\n",
       " ('Sindre Sorhus', '(muukii)'),\n",
       " ('Florian Märkl', '(jwiegley)'),\n",
       " ('Hiroshi Kimura', '(crazy-max)')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of names\n",
    "\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n        geohot /\\n\\n      tinygrad\\n'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "soup.select(\"h1 > a\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = []\n",
    "\n",
    "for repo in soup.select(\"h1 > a\"):\n",
    "    repository.append(repo.get_text().replace(\"\\n\", \"\").replace(\"/\", \"\").replace(\"      \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  geohot tinygrad',\n",
       " '  quantopian zipline',\n",
       " '  mxrch GHunt',\n",
       " '  chubin cheat.sh',\n",
       " '  nidhaloff igel',\n",
       " '  mitmproxy mitmproxy',\n",
       " '  y1ndan genshin-impact-helper',\n",
       " '  python cpython',\n",
       " '  Cuuhomientrung cuuhomientrung',\n",
       " '  blackjack4494 yt-dlc',\n",
       " '  home-assistant core',\n",
       " '  ct-Open-Source tuya-convert',\n",
       " '  teja156 microsoft-teams-class-attender',\n",
       " '  eriklindernoren ML-From-Scratch',\n",
       " '  rajkumardusad Tool-X',\n",
       " '  mlech26l keras-ncp',\n",
       " '  H1R0GH057 Anonymous',\n",
       " '  donnemartin data-science-ipython-notebooks',\n",
       " '  openai gym',\n",
       " '  zongyi-li fourier_neural_operator',\n",
       " '  scikit-learn scikit-learn',\n",
       " '  donnemartin system-design-primer',\n",
       " '  MrMimic data-scientist-roadmap',\n",
       " '  deeppomf DeepCreamPy',\n",
       " '  AlexxIT XiaomiGateway3']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of repositories\n",
    "\n",
    "repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_repo = []\n",
    "\n",
    "for name, repo in zip(new_names, repository):\n",
    "    name_repo.append((name, \"(\" + repo + \")\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('David Tolnay', '(thiserror)'),\n",
       " ('Stephen Celis', '(SQLite.swift)'),\n",
       " ('Henrik Rydgård', '(ppsspp)'),\n",
       " ('Stefan Prodan', '(AspNetCoreRateLimit)'),\n",
       " ('Matthias Urhahn', '(sdmaid-public)'),\n",
       " ('Shreyas Patil', '(NotyKT)'),\n",
       " ('Mr.doob', '(three.js)'),\n",
       " ('Christian Muehlhaeuser', '(gamut)'),\n",
       " ('Anuken', '(Mindustry)'),\n",
       " ('Mathew Sachin', '(Captura)'),\n",
       " ('Laurent', '(joplin)'),\n",
       " ('cketti', '(OkHttpWithContentUri)'),\n",
       " ('Andrei Neagoie', '(ztm-python-cheat-sheet)'),\n",
       " ('Arjay Angeles', '(laravel-datatables)'),\n",
       " ('Zen Monk Alain M. Lafon', '(emacs.d)'),\n",
       " ('yhirose', '(cpp-httplib)'),\n",
       " ('Jared Palmer', '(razzle)'),\n",
       " ('PySimpleGUI', '(PySimpleGUI)'),\n",
       " ('Andrew Gallant', '(ripgrep)'),\n",
       " ('John Arundel', '(script)'),\n",
       " ('Sindre Sorhus', '(awesome)'),\n",
       " ('Florian Märkl', '(chiaki)'),\n",
       " ('Hiroshi Kimura', '(Rideau)'),\n",
       " ('John Wiegley', '(use-package)'),\n",
       " ('CrazyMax', '(WindowsSpyBlocker)')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/File:Walt_Disney_1946.JPG'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "soup.select(\".image\")[0][\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_links = []\n",
    "\n",
    "root = \"https://en.wikipedia.org/\"\n",
    "\n",
    "for disney in soup.select(\".image\"):\n",
    "    disney_links.append(root + disney[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org//wiki/File:Walt_Disney_1946.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Steamboat-willie.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_1935.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:DisneySchiphol1951.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney_Display_Case.JPG',\n",
       " 'https://en.wikipedia.org//wiki/File:Disney1968.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Animation_disc.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:P_vip.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Magic_Kingdom_castle.jpg',\n",
       " 'https://en.wikipedia.org//wiki/File:Video-x-generic.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Blank_television_set.svg',\n",
       " 'https://en.wikipedia.org//wiki/File:Flag_of_the_United_States.svg']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of links\n",
    "\n",
    "disney_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Pythons'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "soup.select(\"li > a\")[0][\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_list = []\n",
    "#mw-content-text > div.mw-parser-output > ul:nth-child(11) > li:nth-child(1) > a\n",
    "for python in soup.select(\"ul > li > a\"):\n",
    "    py_list.append(root + python[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org//wiki/Pythons',\n",
       " 'https://en.wikipedia.org//wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/#Computing',\n",
       " 'https://en.wikipedia.org/#People',\n",
       " 'https://en.wikipedia.org/#Roller_coasters',\n",
       " 'https://en.wikipedia.org/#Vehicles',\n",
       " 'https://en.wikipedia.org/#Weaponry',\n",
       " 'https://en.wikipedia.org/#Other_uses',\n",
       " 'https://en.wikipedia.org/#See_also',\n",
       " 'https://en.wikipedia.org//wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org//wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org//wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org//wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org//wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org//wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org//wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org//wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org//wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org//wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org//wiki/PYTHON',\n",
       " 'https://en.wikipedia.org//wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org//wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org//wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org//wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org//wiki/Cython',\n",
       " 'https://en.wikipedia.org//wiki/Pithon',\n",
       " 'https://en.wikipedia.org//wiki/Category:Disambiguation_pages',\n",
       " 'https://en.wikipedia.org//wiki/Category:Human_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org//wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " 'https://en.wikipedia.org//wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " 'https://en.wikipedia.org//wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " 'https://en.wikipedia.org//wiki/Category:All_article_disambiguation_pages',\n",
       " 'https://en.wikipedia.org//wiki/Category:All_disambiguation_pages',\n",
       " 'https://en.wikipedia.org//wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org//wiki/Special:MyTalk',\n",
       " 'https://en.wikipedia.org//wiki/Special:MyContributions',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " 'https://en.wikipedia.org//wiki/Python',\n",
       " 'https://en.wikipedia.org//wiki/Talk:Python',\n",
       " 'https://en.wikipedia.org//wiki/Python',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Python&action=edit',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Python&action=history',\n",
       " 'https://en.wikipedia.org//wiki/Main_Page',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:Contents',\n",
       " 'https://en.wikipedia.org//wiki/Portal:Current_events',\n",
       " 'https://en.wikipedia.org//wiki/Special:Random',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:About',\n",
       " 'https://en.wikipedia.org///en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://en.wikipedia.org/https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://en.wikipedia.org//wiki/Help:Contents',\n",
       " 'https://en.wikipedia.org//wiki/Help:Introduction',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:Community_portal',\n",
       " 'https://en.wikipedia.org//wiki/Special:RecentChanges',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:File_Upload_Wizard',\n",
       " 'https://en.wikipedia.org//wiki/Special:WhatLinksHere/Python',\n",
       " 'https://en.wikipedia.org//wiki/Special:RecentChangesLinked/Python',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:File_Upload_Wizard',\n",
       " 'https://en.wikipedia.org//wiki/Special:SpecialPages',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Python&oldid=985997919',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Python&action=info',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Special:CiteThisPage&page=Python&id=985997919&wpFormIdentifier=titleform',\n",
       " 'https://en.wikipedia.org/https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " 'https://en.wikipedia.org//w/index.php?title=Python&printable=yes',\n",
       " 'https://en.wikipedia.org/https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://en.wikipedia.org/https://af.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://als.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://en.wikipedia.org/https://az.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://en.wikipedia.org/https://be.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://en.wikipedia.org/https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://en.wikipedia.org/https://da.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://de.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://en.wikipedia.org/https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://en.wikipedia.org/https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://en.wikipedia.org/https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://en.wikipedia.org/https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://en.wikipedia.org/https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://en.wikipedia.org/https://id.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://en.wikipedia.org/https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://en.wikipedia.org/https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://en.wikipedia.org/https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://en.wikipedia.org/https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://en.wikipedia.org/https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://en.wikipedia.org/https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://en.wikipedia.org/https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://en.wikipedia.org/https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://en.wikipedia.org/https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://en.wikipedia.org/https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://en.wikipedia.org/https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://en.wikipedia.org/https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://en.wikipedia.org/https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://en.wikipedia.org/https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://en.wikipedia.org/https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://en.wikipedia.org/https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://en.wikipedia.org///en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " 'https://en.wikipedia.org///creativecommons.org/licenses/by-sa/3.0/',\n",
       " 'https://en.wikipedia.org///foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " 'https://en.wikipedia.org///foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://en.wikipedia.org///www.wikimediafoundation.org/',\n",
       " 'https://en.wikipedia.org/https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:About',\n",
       " 'https://en.wikipedia.org//wiki/Wikipedia:General_disclaimer',\n",
       " 'https://en.wikipedia.org///en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://en.wikipedia.org///en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://en.wikipedia.org/https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://en.wikipedia.org/https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://en.wikipedia.org/https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://en.wikipedia.org/https://wikimediafoundation.org/',\n",
       " 'https://en.wikipedia.org/https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of links\n",
    "\n",
    "py_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "title_change = []\n",
    "\n",
    "for title in soup.select(\".usctitlechanged\"):\n",
    "    title_change.append(title.get_text().replace(\"\\n\", \"\").replace(\"٭\", \"\").strip())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 8 - Aliens and Nationality',\n",
       " 'Title 11 - Bankruptcy',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 18 - Crimes and Criminal Procedure',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 31 - Money and Finance',\n",
       " 'Title 32 - National Guard',\n",
       " \"Title 38 - Veterans' Benefits\",\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 51 - National and Commercial Space Programs']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "most_wanted = []\n",
    "\n",
    "for wanted in soup.select(\"li > h3 > a\"):\n",
    "    most_wanted.append(wanted.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "#soup.select(\"td > b > a\")[0].get_text()\n",
    "\n",
    "date = []\n",
    "\n",
    "for d in soup.select(\"td > b > a\"):\n",
    "    date.append(d.get_text().replace(\"\\xa0\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-11-0717:14:55.8',\n",
       " '2020-11-0716:57:01.3',\n",
       " '2020-11-0716:42:56.0',\n",
       " '2020-11-0716:24:11.0',\n",
       " '2020-11-0716:10:51.8',\n",
       " '2020-11-0716:01:54.0',\n",
       " '2020-11-0715:54:47.3',\n",
       " '2020-11-0715:34:38.0',\n",
       " '2020-11-0715:34:08.9',\n",
       " '2020-11-0715:22:20.0',\n",
       " '2020-11-0715:16:35.9',\n",
       " '2020-11-0715:16:28.6',\n",
       " '2020-11-0715:03:53.9',\n",
       " '2020-11-0715:02:25.3',\n",
       " '2020-11-0715:01:42.5',\n",
       " '2020-11-0715:01:23.0',\n",
       " '2020-11-0714:28:14.4',\n",
       " '2020-11-0714:19:33.0',\n",
       " '2020-11-0714:12:24.0',\n",
       " '2020-11-0714:03:18.1',\n",
       " '2020-11-0713:47:33.0',\n",
       " '2020-11-0713:42:26.0',\n",
       " '2020-11-0712:49:18.0',\n",
       " '2020-11-0712:30:24.0',\n",
       " '2020-11-0712:27:38.1',\n",
       " '2020-11-0712:27:27.9',\n",
       " '2020-11-0712:26:17.5',\n",
       " '2020-11-0712:23:11.1',\n",
       " '2020-11-0712:16:42.3',\n",
       " '2020-11-0712:15:42.6',\n",
       " '2020-11-0712:13:44.5',\n",
       " '2020-11-0712:09:34.0',\n",
       " '2020-11-0712:01:31.9',\n",
       " '2020-11-0711:39:29.9',\n",
       " '2020-11-0711:39:25.7',\n",
       " '2020-11-0711:36:08.5',\n",
       " '2020-11-0711:33:33.0',\n",
       " '2020-11-0711:26:56.0',\n",
       " '2020-11-0711:22:15.0',\n",
       " '2020-11-0711:21:07.0',\n",
       " '2020-11-0711:19:40.4',\n",
       " '2020-11-0711:10:13.4',\n",
       " '2020-11-0710:55:35.0',\n",
       " '2020-11-0710:46:32.1',\n",
       " '2020-11-0710:36:58.0',\n",
       " '2020-11-0710:30:03.9',\n",
       " '2020-11-0710:09:24.0',\n",
       " '2020-11-0710:07:28.9',\n",
       " '2020-11-0709:51:00.0',\n",
       " '2020-11-0709:46:21.1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.select(\"td:nth-child(5)\")[0].get_text()\n",
    "\n",
    "latitude = []\n",
    "\n",
    "for la in soup.select(\"td:nth-child(5)\"):\n",
    "    latitude.append(la.get_text().replace(\"\\xa0\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20.62',\n",
       " '17.96',\n",
       " '37.83',\n",
       " '20.99',\n",
       " '37.87',\n",
       " '36.81',\n",
       " '20.80',\n",
       " '14.74',\n",
       " '47.88',\n",
       " '37.88',\n",
       " '3.21',\n",
       " '46.91',\n",
       " '28.68',\n",
       " '39.03',\n",
       " '25.49',\n",
       " '46.90',\n",
       " '36.18',\n",
       " '61.53',\n",
       " '37.89',\n",
       " '40.53',\n",
       " '37.80',\n",
       " '37.85',\n",
       " '17.45',\n",
       " '39.33',\n",
       " '44.31',\n",
       " '6.69',\n",
       " '20.30',\n",
       " '37.88',\n",
       " '23.01',\n",
       " '46.90',\n",
       " '61.56',\n",
       " '38.26',\n",
       " '61.52',\n",
       " '6.16',\n",
       " '51.39',\n",
       " '37.87',\n",
       " '15.88',\n",
       " '24.50',\n",
       " '37.88',\n",
       " '17.98',\n",
       " '37.87',\n",
       " '15.87',\n",
       " '10.43',\n",
       " '29.34',\n",
       " '15.28',\n",
       " '40.51',\n",
       " '46.90',\n",
       " '37.83',\n",
       " '23.57',\n",
       " '37.87']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.select(\"td:nth-child(6)\")[0].get_text()\n",
    "\n",
    "lat_coordinates = []\n",
    "\n",
    "for c in soup.select(\"td:nth-child(6)\"):\n",
    "    lat_coordinates.append(c.get_text().replace(\"\\xa0\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'S',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'N',\n",
       " 'S',\n",
       " 'N']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = []\n",
    "\n",
    "for lo in soup.select(\"td:nth-child(7)\"):\n",
    "    longitude.append(lo.get_text().replace(\"\\xa0\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['69.09',\n",
       " '66.98',\n",
       " '26.87',\n",
       " '176.68',\n",
       " '26.91',\n",
       " '177.16',\n",
       " '69.19',\n",
       " '119.74',\n",
       " '7.44',\n",
       " '26.65',\n",
       " '130.53',\n",
       " '9.14',\n",
       " '67.33',\n",
       " '25.89',\n",
       " '71.03',\n",
       " '9.15',\n",
       " '96.97',\n",
       " '149.90',\n",
       " '26.46',\n",
       " '176.63',\n",
       " '26.80',\n",
       " '26.72',\n",
       " '99.44',\n",
       " '44.34',\n",
       " '115.17',\n",
       " '126.66',\n",
       " '70.17',\n",
       " '26.95',\n",
       " '70.53',\n",
       " '9.15',\n",
       " '149.91',\n",
       " '42.87',\n",
       " '149.90',\n",
       " '149.04',\n",
       " '179.06',\n",
       " '26.95',\n",
       " '98.23',\n",
       " '121.89',\n",
       " '26.99',\n",
       " '66.85',\n",
       " '26.97',\n",
       " '98.19',\n",
       " '124.92',\n",
       " '71.18',\n",
       " '94.57',\n",
       " '20.85',\n",
       " '9.12',\n",
       " '27.02',\n",
       " '68.77',\n",
       " '26.49']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_coordinates = []\n",
    "\n",
    "for co in soup.select(\"td:nth-child(8)\"):\n",
    "    lon_coordinates.append(co.get_text().replace(\"\\xa0\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'W',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E',\n",
       " 'W',\n",
       " 'W',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'W',\n",
       " 'E']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.select(\".tb_region\")[0].get_text()\n",
    "\n",
    "region = []\n",
    "\n",
    "for r in soup.select(\".tb_region\"):\n",
    "    region.append(r.get_text().replace(\"\\xa0\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TARAPACA, CHILE',\n",
       " 'PUERTO RICO',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'FIJI REGION',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'OFF E. COAST OF N. ISLAND, N.Z.',\n",
       " 'TARAPACA, CHILE',\n",
       " 'LUZON, PHILIPPINES',\n",
       " 'FRANCE-GERMANY BORDER REGION',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'SERAM, INDONESIA',\n",
       " 'SWITZERLAND',\n",
       " 'LA RIOJA, ARGENTINA',\n",
       " 'AEGEAN SEA',\n",
       " 'OFF COAST OF ANTOFAGASTA, CHILE',\n",
       " 'SWITZERLAND',\n",
       " 'OKLAHOMA',\n",
       " 'SOUTHERN ALASKA',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'NORTH ISLAND OF NEW ZEALAND',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'GUERRERO, MEXICO',\n",
       " 'TURKEY-IRAN BORDER REGION',\n",
       " 'SOUTHERN IDAHO',\n",
       " 'MINDANAO, PHILIPPINES',\n",
       " 'OFFSHORE TARAPACA, CHILE',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'OFFSHORE ANTOFAGASTA, CHILE',\n",
       " 'SWITZERLAND',\n",
       " 'SOUTHERN ALASKA',\n",
       " 'EASTERN TURKEY',\n",
       " 'SOUTHERN ALASKA',\n",
       " 'NEW BRITAIN REGION, P.N.G.',\n",
       " 'ANDREANOF ISLANDS, ALEUTIAN IS.',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'OFFSHORE OAXACA, MEXICO',\n",
       " 'TAIWAN',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'PUERTO RICO',\n",
       " 'DODECANESE ISLANDS, GREECE',\n",
       " 'OFFSHORE OAXACA, MEXICO',\n",
       " 'LEYTE, PHILIPPINES',\n",
       " 'COQUIMBO, CHILE',\n",
       " 'OFF COAST OF OAXACA, MEXICO',\n",
       " 'ALBANIA',\n",
       " 'SWITZERLAND',\n",
       " 'WESTERN TURKEY',\n",
       " 'ANTOFAGASTA, CHILE',\n",
       " 'DODECANESE ISLANDS, GREECE']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques = pd.DataFrame({\"date\":date,\n",
    "                            \"latitude\":latitude,\n",
    "                            \"lat_coordinates\":lat_coordinates,\n",
    "                            \"longitude\":longitude,\n",
    "                            \"lon_coordinates\":lon_coordinates,\n",
    "                            \"region\":region\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques[\"latitudes\"] = earthquaques[\"latitude\"] + earthquaques[\"lat_coordinates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques.drop(\"lat_coordinates\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques[\"longitudes\"] = earthquaques[\"longitude\"] + earthquaques[\"lon_coordinates\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques.drop(\"lon_coordinates\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques.drop([\"latitude\", \"longitude\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'region', 'latitudes', 'longitudes'], dtype='object')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquaques.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquaques = earthquaques[['date', 'latitudes', 'longitudes', 'region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>latitudes</th>\n",
       "      <th>longitudes</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-0717:41:55.0</td>\n",
       "      <td>20.62S</td>\n",
       "      <td>69.09W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-0717:26:05.3</td>\n",
       "      <td>17.96N</td>\n",
       "      <td>66.98W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-0717:23:54.4</td>\n",
       "      <td>37.83N</td>\n",
       "      <td>26.87E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-0717:17:02.1</td>\n",
       "      <td>20.99S</td>\n",
       "      <td>176.68W</td>\n",
       "      <td>FIJI REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-0717:15:16.2</td>\n",
       "      <td>37.87N</td>\n",
       "      <td>26.91E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-0716:57:01.3</td>\n",
       "      <td>36.81S</td>\n",
       "      <td>177.16E</td>\n",
       "      <td>OFF E. COAST OF N. ISLAND, N.Z.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-0716:42:56.0</td>\n",
       "      <td>20.80S</td>\n",
       "      <td>69.19W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-0716:24:11.0</td>\n",
       "      <td>14.74N</td>\n",
       "      <td>119.74E</td>\n",
       "      <td>LUZON, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-0716:21:01.0</td>\n",
       "      <td>47.88N</td>\n",
       "      <td>7.44E</td>\n",
       "      <td>FRANCE-GERMANY BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-0716:10:51.8</td>\n",
       "      <td>37.88N</td>\n",
       "      <td>26.65E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-11-0716:01:54.0</td>\n",
       "      <td>3.21S</td>\n",
       "      <td>130.53E</td>\n",
       "      <td>SERAM, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-11-0715:54:47.3</td>\n",
       "      <td>46.91N</td>\n",
       "      <td>9.14E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-11-0715:34:38.0</td>\n",
       "      <td>28.68S</td>\n",
       "      <td>67.33W</td>\n",
       "      <td>LA RIOJA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-11-0715:34:08.9</td>\n",
       "      <td>39.03N</td>\n",
       "      <td>25.89E</td>\n",
       "      <td>AEGEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-11-0715:22:20.0</td>\n",
       "      <td>25.49S</td>\n",
       "      <td>71.03W</td>\n",
       "      <td>OFF COAST OF ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-11-0715:16:35.9</td>\n",
       "      <td>46.90N</td>\n",
       "      <td>9.15E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-11-0715:16:28.6</td>\n",
       "      <td>36.18N</td>\n",
       "      <td>96.97W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-11-0715:03:53.9</td>\n",
       "      <td>61.53N</td>\n",
       "      <td>149.90W</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-11-0715:02:25.3</td>\n",
       "      <td>37.89N</td>\n",
       "      <td>26.46E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-11-0715:01:42.5</td>\n",
       "      <td>40.53S</td>\n",
       "      <td>176.63E</td>\n",
       "      <td>NORTH ISLAND OF NEW ZEALAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date latitudes longitudes                           region\n",
       "0   2020-11-0717:41:55.0    20.62S     69.09W                  TARAPACA, CHILE\n",
       "1   2020-11-0717:26:05.3    17.96N     66.98W                      PUERTO RICO\n",
       "2   2020-11-0717:23:54.4    37.83N     26.87E       DODECANESE ISLANDS, GREECE\n",
       "3   2020-11-0717:17:02.1    20.99S    176.68W                      FIJI REGION\n",
       "4   2020-11-0717:15:16.2    37.87N     26.91E       DODECANESE ISLANDS, GREECE\n",
       "5   2020-11-0716:57:01.3    36.81S    177.16E  OFF E. COAST OF N. ISLAND, N.Z.\n",
       "6   2020-11-0716:42:56.0    20.80S     69.19W                  TARAPACA, CHILE\n",
       "7   2020-11-0716:24:11.0    14.74N    119.74E               LUZON, PHILIPPINES\n",
       "8   2020-11-0716:21:01.0    47.88N      7.44E     FRANCE-GERMANY BORDER REGION\n",
       "9   2020-11-0716:10:51.8    37.88N     26.65E       DODECANESE ISLANDS, GREECE\n",
       "10  2020-11-0716:01:54.0     3.21S    130.53E                 SERAM, INDONESIA\n",
       "11  2020-11-0715:54:47.3    46.91N      9.14E                      SWITZERLAND\n",
       "12  2020-11-0715:34:38.0    28.68S     67.33W              LA RIOJA, ARGENTINA\n",
       "13  2020-11-0715:34:08.9    39.03N     25.89E                       AEGEAN SEA\n",
       "14  2020-11-0715:22:20.0    25.49S     71.03W  OFF COAST OF ANTOFAGASTA, CHILE\n",
       "15  2020-11-0715:16:35.9    46.90N      9.15E                      SWITZERLAND\n",
       "16  2020-11-0715:16:28.6    36.18N     96.97W                         OKLAHOMA\n",
       "17  2020-11-0715:03:53.9    61.53N    149.90W                  SOUTHERN ALASKA\n",
       "18  2020-11-0715:02:25.3    37.89N     26.46E       DODECANESE ISLANDS, GREECE\n",
       "19  2020-11-0715:01:42.5    40.53S    176.63E      NORTH ISLAND OF NEW ZEALAND"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquaques.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "#soup.select(\"strong\")[0].get_text()\n",
    "\n",
    "language = []\n",
    "\n",
    "for lan in soup.select(\"strong\"):\n",
    "       language.append(lan.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Free Encyclopedia'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDownload Wikipedia for Android or iOS\\n\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language.pop(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'Español',\n",
       " '日本語',\n",
       " 'Deutsch',\n",
       " 'Русский',\n",
       " 'Français',\n",
       " 'Italiano',\n",
       " '中文',\n",
       " 'Português',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = []\n",
    "\n",
    "for art in soup.select(\"small\"):\n",
    "       article.append(art.get_text().replace(\"\\xa0\", \"\").replace(\"+\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "del article[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6183000 articles',\n",
       " '1637000 artículos',\n",
       " '1235000 記事',\n",
       " '2495000 Artikel',\n",
       " '1672000 статей',\n",
       " '2262000 articles',\n",
       " '1645000 voci',\n",
       " '1155000 條目',\n",
       " '1045000 artigos',\n",
       " '1435000 haseł']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.DataFrame({\"language\":language,\n",
    "                    \"article\":article})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>6183000 articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Español</td>\n",
       "      <td>1637000 artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1235000 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2495000 Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1672000 статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2262000 articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1645000 voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1155000 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1045000 artigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1435000 haseł</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language            article\n",
       "0    English   6183000 articles\n",
       "1    Español  1637000 artículos\n",
       "2        日本語         1235000 記事\n",
       "3    Deutsch    2495000 Artikel\n",
       "4    Русский     1672000 статей\n",
       "5   Français   2262000 articles\n",
       "6   Italiano       1645000 voci\n",
       "7         中文         1155000 條目\n",
       "8  Português    1045000 artigos\n",
       "9     Polski      1435000 haseł"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language and articles\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for data in soup.select(\"h3 > a\"):\n",
    "    datasets.append(data.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of datasets\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "#soup.select(\"table> tbody > tr > td > a\")[0].get_text()\n",
    "\n",
    "languages = []\n",
    "\n",
    "for l in soup.select(\"td > a\"):\n",
    "    languages.append(l.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del languages[1:3]\n",
    "#del languages[2:4]\n",
    "#del languages[3:5]\n",
    "#del languages[4:7]\n",
    "#del languages[5:7]\n",
    "#del languages[6:8]\n",
    "#del languages[7:9]\n",
    "#del languages[8:10]\n",
    "#del languages[9:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = pd.DataFrame({\"top_10_lan\":languages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_10_lan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         top_10_lan\n",
       "0  Mandarin Chinese\n",
       "1           Spanish\n",
       "2           English\n",
       "3             Hindi\n",
       "4           Bengali\n",
       "5        Portuguese\n",
       "6           Russian\n",
       "7          Japanese\n",
       "8   Western Punjabi\n",
       "9           Marathi"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 10 languages\n",
    "\n",
    "top10.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# selecting all movie names\n",
    "\n",
    "movie_name = []\n",
    "\n",
    "for m in soup.select(\"tr > td.titleColumn > a\"):\n",
    "    movie_name.append(m.get_text())\n",
    "\n",
    "# selecting all initial release\n",
    "\n",
    "initial_release = []\n",
    "\n",
    "for i in soup.select():\n",
    "    initial_release.append(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Top Box Office'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select(\"a:nth-child(6)\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
